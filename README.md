# Web House Scraper

Aplicaci√≥n para scrapear propiedades inmobiliarias de Argenprop y Zonaprop, procesar la informaci√≥n con un **sistema h√≠brido** (extracci√≥n estructurada + LLM local) y guardarla en una base de datos SQLite.

## Caracter√≠sticas

- üîç Scraping de propiedades desde Argenprop y Zonaprop
- üéØ **Sistema H√≠brido de Extracci√≥n**:
  - Extracci√≥n estructurada con regex/CSS para campos num√©ricos (100% precisi√≥n)
  - LLM (Ollama) solo para campos complejos (direcciones, descripciones)
- ‚öôÔ∏è **Configuraci√≥n por Sitio**: Patrones definidos en `data/site_configs.json` (sin modificar c√≥digo)
- ü§ñ Procesamiento inteligente con LLM (Ollama o Google Gemini)
- üíæ Almacenamiento en SQLite con funcionalidad **UPSERT** (insert/update)
- üõ°Ô∏è Manejo de anti-scraping mediante Playwright
- üì¶ **Estructura Profesional**: Organizado seg√∫n mejores pr√°cticas de Python

## Requisitos

- Python 3.8+
- Ollama instalado y corriendo localmente (por defecto)
- Sistema operativo: Linux, macOS, Windows

## Instalaci√≥n

1. **Clonar el repositorio**

```bash
cd /home/seba/Documentos/Data\ Science\ Projects/web-house-scraper
```

2. **Crear entorno virtual**

```bash
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

3. **Instalar dependencias**

```bash
pip install -r requirements.txt
playwright install chromium
```

4. **Configurar variables de entorno**

```bash
cp .env.example .env
# Editar .env si es necesario
```

5. **Asegurar que Ollama est√© corriendo**

```bash
ollama serve  # En otra terminal
ollama pull deepseek-r1:latest  # Descargar el modelo
```

## Estructura del Proyecto

```
web-house-scraper/
‚îÇ
‚îú‚îÄ‚îÄ üìÅ src/                          # C√≥digo fuente
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ core/                     # L√≥gica principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scraper.py              # Web scraping
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_processor.py        # Procesamiento h√≠brido LLM
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ structured_extractor.py # Extracci√≥n estructurada
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ database/                 # Capa de datos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database.py             # SQLite con UPSERT
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ utils/                    # Utilidades
‚îÇ       ‚îî‚îÄ‚îÄ config.py               # Configuraci√≥n central
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/                         # Datos y configuraciones
‚îÇ   ‚îú‚îÄ‚îÄ properties.db               # Base de datos SQLite
‚îÇ   ‚îú‚îÄ‚îÄ links-to-scrap.md          # URLs a procesar
‚îÇ   ‚îî‚îÄ‚îÄ site_configs.json          # Patrones de extracci√≥n por sitio
‚îÇ
‚îú‚îÄ‚îÄ üìÅ scripts/                      # Scripts ejecutables
‚îÇ   ‚îú‚îÄ‚îÄ main.py                     # Script principal
‚îÇ   ‚îî‚îÄ‚îÄ view_db.py                  # Visualizar base de datos
‚îÇ
‚îú‚îÄ‚îÄ üìÅ tests/                        # Tests (preparado para el futuro)
‚îÇ
‚îú‚îÄ‚îÄ .env                             # Variables de entorno (privado)
‚îú‚îÄ‚îÄ .env.example                     # Ejemplo de configuraci√≥n
‚îú‚îÄ‚îÄ requirements.txt                 # Dependencias Python
‚îî‚îÄ‚îÄ README.md                        # Esta documentaci√≥n
```

## Uso R√°pido

### 1. Agregar URLs

Edita `data/links-to-scrap.md` y agrega URLs (una por l√≠nea):

```
https://www.zonaprop.com.ar/propiedades/...
https://www.argenprop.com/departamento-en-venta...
```

### 2. Ejecutar el scraper

```bash
python scripts/main.py
```

### 3. Ver resultados

```bash
python scripts/view_db.py
```

O acceder directamente a la base de datos:

```bash
sqlite3 data/properties.db "SELECT * FROM properties;"
```

## Configuraci√≥n

### Campos Extra√≠dos

El sistema extrae **21 campos** autom√°ticamente (definidos en `src/utils/config.py`):

**B√°sicos:**
- `tipo_operacion` (venta/alquiler)
- `tipo_inmueble` (casa/departamento)
- `direccion`
- `barrio`

**Medidas:**
- `metros_cuadrados_cubiertos`
- `metros_cuadrados_totales`
- `precio`
- `moneda` (USD/ARS)

**Distribuci√≥n:**
- `cantidad_dormitorios`
- `cantidad_banos`
- `cantidad_ambientes`

**Caracter√≠sticas booleanas:**
- `tiene_patio`
- `tiene_quincho`
- `tiene_pileta`
- `tiene_cochera`
- `tiene_balcon`
- `tiene_terraza`

**Detalles adicionales:**
- `piso` (PB, 1, 2, 3... o 0 para casas)
- `orientacion` (Norte, Sur, Este, Oeste)
- `antiguedad`
- `descripcion_breve`

### Variables de Entorno (.env)

```bash
LLM_PROVIDER=ollama                      # ollama o google
OLLAMA_MODEL=deepseek-r1:latest         # Modelo de Ollama
OLLAMA_BASE_URL=http://localhost:11434  # URL base de Ollama
GOOGLE_API_KEY=                         # Para usar Gemini (opcional)
GEMINI_MODEL=gemini-pro                 # Modelo de Gemini
```

## Sistema H√≠brido de Extracci√≥n

La aplicaci√≥n usa un enfoque de **dos pasos** para m√°xima precisi√≥n y eficiencia:

### Paso 1: Extracci√≥n Estructurada

Primero extrae campos usando **patrones regex/CSS** definidos en `data/site_configs.json`:

‚úÖ **100% precisi√≥n** en:
- Precio
- Moneda  
- Metros cuadrados (cubiertos y totales)
- Cantidad de dormitorios/ba√±os
- Tipo de operaci√≥n (venta/alquiler)

### Paso 2: Procesamiento LLM

Luego usa el **LLM solo para campos complejos**:
- Direcci√≥n completa
- Barrio/zona
- Caracter√≠sticas (patio, quincho, pileta, balc√≥n, terraza, etc.)
- Tipo de inmueble
- Orientaci√≥n
- Piso
- Descripci√≥n breve
- Antig√ºedad

### Ventajas

- ‚ö° **100% precisi√≥n** en campos num√©ricos
- üöÄ **M√°s r√°pido**: LLM procesa solo ~40% de los campos
- üîß **Mantenible**: Patrones en JSON, no en c√≥digo
- üìà **Escalable**: F√°cil agregar nuevos sitios

### Agregar Nuevo Sitio

Edita `data/site_configs.json` y agrega configuraci√≥n:

```json
{
  "nuevositio.com": {
    "name": "Nuevo Sitio",
    "structured_fields": ["precio", "moneda", "cantidad_dormitorios"],
    "llm_fields": ["direccion", "barrio", "descripcion_breve"],
    "patterns": {
      "precio": {
        "type": "regex",
        "pattern": "(?:USD|ARS)\\s*([\\d.,]+)",
        "search_in": "text"
      },
      "cantidad_dormitorios": {
        "type": "css",
        "selector": ".bedrooms-count",
        "attribute": "text"
      }
    }
  }
}
```

## Funcionalidad UPSERT

El sistema implementa **UPSERT** (Update or Insert) autom√°tico:

### Comportamiento

Cuando ejecutas la app con un link que ya existe en la base de datos:

- ‚úÖ **Se actualiza** la propiedad con los nuevos datos extra√≠dos
- ‚úÖ **Se actualiza** el timestamp `scraped_at` autom√°ticamente  
- ‚úÖ El terminal muestra claramente: `üîÑ Updated property`
- ‚úÖ El resumen final distingue entre "New properties" y "Updated properties"

### Ejemplo de Output

```
[1/5] Processing: https://www.zonaprop.com.ar/...
INFO:src.database.database:üîÑ Updated property: https://www.zonaprop.com.ar/...
INFO:__main__:‚úì Successfully updated existing property

SUMMARY
================================================================================
Total URLs:           5
New properties:       0
Updated properties:   5    üëà Todas actualizadas
Failed:               0
```

### Casos de Uso

- üìä Actualizar precios que cambian
- üîÑ Refrescar caracter√≠sticas modificadas
- üìà Hacer seguimiento temporal de propiedades
- üîç Monitoreo continuo del mercado

## Soluci√≥n de Problemas

### Error: "externally-managed-environment"

**Soluci√≥n**: Usar entorno virtual (ver instalaci√≥n).

### Error: "Links file not found"

**Causa**: Archivo `data/links-to-scrap.md` no existe o est√° vac√≠o.

**Soluci√≥n**: 
```bash
echo "https://www.zonaprop.com.ar/..." > data/links-to-scrap.md
```

### Timeout en scraping

**Causa**: Algunas p√°ginas pueden tardar en cargar.

**Soluci√≥n**: Ajustar timeout en `src/core/scraper.py` (l√≠nea ~40).

### LLM no responde

**Verificar que Ollama est√© corriendo**:
```bash
ollama list  # Ver modelos instalados
ollama serve # Iniciar servidor
```

### Error 403 Forbidden

**Normal**: Zonaprop usa protecci√≥n anti-scraping.

**Soluci√≥n autom√°tica**: El sistema usa Playwright como fallback (ya implementado).

## Notas Importantes

- üîí **Anti-scraping**: Zonaprop usa protecci√≥n 403 ‚Üí Se resuelve autom√°ticamente con Playwright
- ü§ñ **LLM Local**: Aseg√∫rate de que Ollama est√© corriendo: `ollama serve`
- üì¶ **Modelo**: Por defecto usa `deepseek-r1:latest` ‚Üí Desc√°rgalo con: `ollama pull deepseek-r1:latest`
- üí∞ **APIs Pagas**: Para usar Google Gemini, configura `GOOGLE_API_KEY` en `.env`
- üîÑ **UPSERT**: Re-ejecutar con los mismos links **actualiza** los datos autom√°ticamente
- üìÅ **Estructura**: Proyecto organizado seg√∫n Python best practices (src/, data/, scripts/)

## Ejemplos de Ejecuci√≥n

### Activar entorno y ejecutar

```bash
cd /home/seba/Documentos/Data\ Science\ Projects/web-house-scraper
source venv/bin/activate
python scripts/main.py
```

### Ver propiedades formateadas

```bash
python scripts/view_db.py
```

### Consulta SQL directa

```bash
sqlite3 data/properties.db
> SELECT direccion, precio, moneda, cantidad_dormitorios FROM properties;
> .quit
```

## Contribuir

Para contribuir al proyecto:

1. Crear feature branch
2. Agregar tests en `tests/`
3. Actualizar documentaci√≥n si es necesario
4. Enviar pull request

## Licencia

MIT
